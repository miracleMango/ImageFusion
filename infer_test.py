import os
import torch
import numpy as np
from PIL import Image
from torchvision import transforms
from models.model import ImageFusionNetworkWithSourcePE  # å¯¼å…¥ä½ çš„æ¨¡å‹


# -------------------------- é…ç½®å‚æ•°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰ --------------------------
WEIGHT_PATH = "./saved_models/best_model_loss_0.1657.pth"
TESTSET_ROOT = "./datasets/M3FD_Fusion_test"  # ã€ä¿®æ”¹ä¸ºä½ çš„æµ‹è¯•é›†æ ¹ç›®å½•ã€‘
IR_SUB_DIR = "Ir"  # çº¢å¤–å­ç›®å½•
VIS_SUB_DIR = "Vis"  # å¯è§å…‰å­ç›®å½•
OUTPUT_DIR = "./results/fusion_results_test"
PATCH_SIZE = (128, 128)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs(OUTPUT_DIR, exist_ok=True)


# -------------------------- æ ¸å¿ƒå·¥å…·å‡½æ•°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰ --------------------------
def split_image_to_patches(img, patch_size=PATCH_SIZE, is_ir=False):
    """ç®€å•åˆ†å—ï¼Œä¸é‡å """
    img_np = np.array(img)
    img_h, img_w = img_np.shape[:2]
    patch_h, patch_w = patch_size
    
    # è®¡ç®—è¡Œæ•°å’Œåˆ—æ•°
    num_rows = img_h // patch_h + (1 if img_h % patch_h > 0 else 0)
    num_cols = img_w // patch_w + (1 if img_w % patch_w > 0 else 0)
    
    patches = []
    coords = []
    
    for row in range(num_rows):
        for col in range(num_cols):
            y_start = row * patch_h
            x_start = col * patch_w
            y_end = min(y_start + patch_h, img_h)
            x_end = min(x_start + patch_w, img_w)
            
            patch = img_np[y_start:y_end, x_start:x_end]
            
            # å¡«å……åˆ°å›ºå®šå¤§å°
            pad_h = patch_h - patch.shape[0] if patch.shape[0] < patch_h else 0
            pad_w = patch_w - patch.shape[1] if patch.shape[1] < patch_w else 0
            if pad_h > 0 or pad_w > 0:
                if is_ir:
                    pad_width = [(0, pad_h), (0, pad_w)]
                else:
                    pad_width = [(0, pad_h), (0, pad_w), (0, 0)]
                patch = np.pad(patch, pad_width, mode="constant", constant_values=0)
            
            # è½¬æ¢ä¸ºPILå›¾åƒ
            if is_ir:
                patch_img = Image.fromarray(patch, mode="L")
            else:
                patch_img = Image.fromarray(patch, mode="RGB")
            patches.append(patch_img)
            coords.append((y_start, y_end, x_start, x_end))
    
    return patches, coords, (img_h, img_w)


def merge_patches_to_full(patches, coords, original_size, is_ir=False):
    """ç®€å•æ‹¼æ¥ï¼Œç›´æ¥æ”¾ç½®"""
    original_h, original_w = original_size
    
    # åˆå§‹åŒ–ç»“æœæ•°ç»„
    if is_ir:
        full_img = np.zeros((original_h, original_w), dtype=np.uint8)
    else:
        full_img = np.zeros((original_h, original_w, 3), dtype=np.uint8)
    
    for patch, (y_s, y_e, x_s, x_e) in zip(patches, coords):
        patch_np = np.array(patch, dtype=np.uint8)
        h_patch = y_e - y_s
        w_patch = x_e - x_s
        
        # è£å‰ªå›åŸå§‹å¤§å°ï¼ˆå»é™¤å¡«å……éƒ¨åˆ†ï¼‰
        patch_cropped = patch_np[:h_patch, :w_patch]
        
        # ç›´æ¥æ”¾ç½®åˆ°å¯¹åº”ä½ç½®
        if is_ir:
            full_img[y_s:y_e, x_s:x_e] = patch_cropped
        else:
            full_img[y_s:y_e, x_s:x_e, :] = patch_cropped
    
    # è½¬æ¢ä¸ºPILå›¾åƒ
    if is_ir:
        return Image.fromarray(full_img, mode="L")
    else:
        return Image.fromarray(full_img)


# æ‹†åˆ†transformï¼šä¿æŒä¸å˜
def get_ir_transform():
    return transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5], std=[0.5])
    ])


def get_vis_transform():
    return transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])


def post_process(tensor, is_ir=False):
    """åå¤„ç†å‡½æ•°"""
    tensor = tensor.detach().cpu()
    tensor = (tensor + 1.0) / 2.0
    tensor = torch.clamp(tensor, 0.0, 1.0)

    if is_ir:
        img = tensor.squeeze(0).numpy()
        img = (img * 255).astype(np.uint8)
        return Image.fromarray(img, mode="L")
    else:
        img = tensor.permute(1, 2, 0).numpy()
        img = (img * 255).astype(np.uint8)
        return Image.fromarray(img)


# -------------------------- å•å¼ å›¾ç‰‡å¯¹æ¨ç†å‡½æ•°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰ --------------------------
def infer_single_pair(ir_path, vis_path, output_dir):
    basename = os.path.basename(ir_path)
    fusion_path = os.path.join(output_dir, basename)  # ç›´æ¥åœ¨è¾“å‡ºç›®å½•ï¼Œä½¿ç”¨åŸå§‹æ–‡ä»¶å

    # åŠ è½½æ¨¡å‹ï¼ˆä»…åŠ è½½ä¸€æ¬¡ï¼‰
    global model
    if "model" not in globals():
        model = ImageFusionNetworkWithSourcePE(
            vis_img_channels=3,
            ir_img_channels=1,
            feature_channels=64,
            num_heads=16,
            use_position_encoding=True
        ).to(DEVICE)
        checkpoint = torch.load(WEIGHT_PATH, map_location=DEVICE, weights_only=False)
        if "model_state_dict" in checkpoint:
            model.load_state_dict(checkpoint["model_state_dict"])
        else:
            model.load_state_dict(checkpoint)
        model.eval()
        print(f"âœ… æ¨¡å‹å·²åŠ è½½ï¼Œå¼€å§‹å¤„ç†å›¾ç‰‡å¯¹ï¼š{basename}")

    # è¯»å–å›¾åƒ
    ir_img_full = Image.open(ir_path).convert("L")
    vis_img_full = Image.open(vis_path).convert("RGB")

    # ç®€å•åˆ†å—
    ir_patches, coords, original_size = split_image_to_patches(ir_img_full, is_ir=True)
    vis_patches, _, _ = split_image_to_patches(vis_img_full, is_ir=False)

    # åŠ è½½transform
    ir_transform = get_ir_transform()
    vis_transform = get_vis_transform()

    fusion_patches = []
    with torch.no_grad():
        for ir_patch, vis_patch in zip(ir_patches, vis_patches):
            # é¢„å¤„ç†
            ir_tensor = ir_transform(ir_patch).unsqueeze(0).to(DEVICE)
            vis_tensor = vis_transform(vis_patch).unsqueeze(0).to(DEVICE)

            # å¼ºåˆ¶ä¿®æ­£çº¢å¤–é€šé“æ•°å’Œå°ºå¯¸
            if ir_tensor.shape[1] != 1:
                ir_tensor = ir_tensor[:, 0:1, :, :]
            if ir_tensor.shape[2:] != (128, 128):
                ir_tensor = torch.nn.functional.interpolate(ir_tensor, size=(128, 128), mode="nearest")

            # æ¨¡å‹æ¨ç†
            outputs = model(ir_tensor, vis_tensor)

            # åå¤„ç†
            fusion_patches.append(post_process(outputs["img_fusion_pred"][0], is_ir=False))

    # ç®€å•æ‹¼æ¥
    full_fusion = merge_patches_to_full(fusion_patches, coords, original_size, is_ir=False)

    # ä¿å­˜èåˆå›¾
    full_fusion.save(fusion_path)
    
    return basename, fusion_path


# -------------------------- æ‰¹é‡å¤„ç†æµ‹è¯•é›† --------------------------
def batch_process_testset():
    ir_dir = os.path.join(TESTSET_ROOT, IR_SUB_DIR)
    vis_dir = os.path.join(TESTSET_ROOT, VIS_SUB_DIR)
    ir_filenames = [f for f in os.listdir(ir_dir) if f.endswith((".png", ".jpg", ".jpeg"))]
    vis_filenames = [f for f in os.listdir(vis_dir) if f.endswith((".png", ".jpg", ".jpeg"))]

    common_filenames = list(set(ir_filenames) & set(vis_filenames))
    if not common_filenames:
        print("âŒ æœªæ‰¾åˆ°åŒ¹é…çš„çº¢å¤–-å¯è§å…‰å›¾ç‰‡å¯¹ï¼è¯·æ£€æŸ¥æ–‡ä»¶åã€‚")
        return
    print(f"âœ… æ‰¾åˆ° {len(common_filenames)} å¯¹åŒ¹é…çš„å›¾ç‰‡ï¼Œå¼€å§‹æ‰¹é‡å¤„ç†...")

    for idx, filename in enumerate(common_filenames, 1):
        ir_path = os.path.join(ir_dir, filename)
        vis_path = os.path.join(vis_dir, filename)
        basename, fusion_path = infer_single_pair(ir_path, vis_path, OUTPUT_DIR)
        print(f"ğŸ”§ å·²å®Œæˆ {idx}/{len(common_filenames)}ï¼š{basename}ï¼Œèåˆå›¾ä¿å­˜è‡³ï¼š{fusion_path}")

    print(f"\nâœ… æ‰¹é‡å¤„ç†å®Œæˆï¼")
    print(f"   - æ‰€æœ‰èåˆå›¾ä¿å­˜è‡³ï¼š{OUTPUT_DIR}")


# -------------------------- è¿è¡Œ --------------------------
if __name__ == "__main__":
    batch_process_testset()